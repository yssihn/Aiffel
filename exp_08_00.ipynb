{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351e54f3",
   "metadata": {},
   "source": [
    "# exp 8 뉴스 요약봇 만들기\n",
    "## 작업파일 링크 :&nbsp;&nbsp;\n",
    "\n",
    "### 평가기준\n",
    "### 1. Abstractive 모델 구성을 위한 텍스트 전처리 단계가 체계적으로 진행되었다.\n",
    "\n",
    "분석단계, 정제단계, 정규화와 불용어 제거, 데이터셋 분리, 인코딩 과정이 빠짐없이 체계적으로 진행되었다.\n",
    "#### 2. 텍스트 요약모델이 성공적으로 학습되었음을 확인하였다.\n",
    "\n",
    "모델학습이 안정적으로 수렴되었음을 그래프를 통해 확인하였으며, 실제 요약문과 유사한 요약문장을 얻을 수 있었다.\n",
    "#### 3. Extractive 요약을 시도해 보고 Abstractive 요약 결과과 함께 비교해 보았다.\n",
    "\n",
    "두 요약 결과를 문법완성도 측면과 핵심단어 포함 측면으로 나누어 비교분석 결과를 제시하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967e133",
   "metadata": {},
   "source": [
    "***\n",
    "## 긴 문장을 짧게 요약해 주는 텍스트 요약기를 만들기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e59735",
   "metadata": {},
   "source": [
    "***\n",
    "### 1. 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a806e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☢️\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "print(\"☢️\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f5a708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332346ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84816</th>\n",
       "      <td>I have been getting the Coffee People Extra Bo...</td>\n",
       "      <td>Need to brew small cup size.  Check the price!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12284</th>\n",
       "      <td>These had been my dog's favorite treats. BUT w...</td>\n",
       "      <td>Vet says NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12271</th>\n",
       "      <td>I bought these from a large chain pet store. a...</td>\n",
       "      <td>do not buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88202</th>\n",
       "      <td>I have a one and two year old who just love th...</td>\n",
       "      <td>My kids love these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55583</th>\n",
       "      <td>What a delicious breakfast cereal! These aren'...</td>\n",
       "      <td>Mighty Tasty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "84816  I have been getting the Coffee People Extra Bo...   \n",
       "12284  These had been my dog's favorite treats. BUT w...   \n",
       "12271  I bought these from a large chain pet store. a...   \n",
       "88202  I have a one and two year old who just love th...   \n",
       "55583  What a delicious breakfast cereal! These aren'...   \n",
       "\n",
       "                                              Summary  \n",
       "84816  Need to brew small cup size.  Check the price!  \n",
       "12284                                     Vet says NO  \n",
       "12271                                      do not buy  \n",
       "88202                              My kids love these  \n",
       "55583                                    Mighty Tasty  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "\n",
    "# 전체 데이터 중 Summary 열과 Text 열만 훈련에 사용할 거라, 이 두 개의 열만 별도로 저장\n",
    "data = data[['Text','Summary']]\n",
    "data.head()\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3373d",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. 데이터 전처리 하기 - (1) 데이터 정리하기\n",
    "빈칸으로 존재하는 null 데이터, \n",
    "의미는 같지만 다른 식으로 작성된 글 같은 중복 항목과 같은 학습할 때 방해가 되는 데이터 정리하기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc8cca",
   "metadata": {},
   "source": [
    "***\n",
    "#### ⦿ 중복 샘플과 NULL 값이 존재하는 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8353be63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79afae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임의 drop_duplicates()를 사용하면, 손쉽게 중복 샘플을 제거\n",
    "\n",
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14fbcefd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임에 Null 값이 있는지 확인하는 방법은 .isnull().sum()을 사용\n",
    "\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6969feff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임에서 Null을 제거할 때는 dropna() 함수를 사용\n",
    "\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1862f0",
   "metadata": {},
   "source": [
    "#### ⦿ 텍스트 정규화와 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16704f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "# 단어들 중에서는 같은 의미인데도 다른 표현으로 쓰여 마치 다른 단어들처럼 간주되는 경우가 있음.\n",
    "# 예) it'll은 it will과 같고, mustn't과 must not은 사실 같은 표현임.\n",
    "# 기계 학습 전에 미리 같은 표현으로 통일시켜주는 것이 기계의 연산량을 줄일 수 있는 방법임.\n",
    "# 이러한 방법론을 텍스트 처리에서는 텍스트 정규화(text normalization) 라고 한다. \n",
    "\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5be0763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 일반적으로 텍스트에는 자주 등장하지만 자연어 처리를 할 때 실질적으로 별 도움이 되지 않는 단어들이 존재. \n",
    "# 이를 불용어(stopwords)라함. 때로는 불용어를 제거하는 것이 자연어 처리의 성능을 높이는 방법일 수 있\u001d",
    ". \n",
    "# 여기서는 NLTK에서 제공하는 불용어 리스트를 참조해, 샘플에서 불용어를 제거\n",
    "\n",
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a9f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☢️\n"
     ]
    }
   ],
   "source": [
    "# NLTK를 이용하여 불용어를 제거, \n",
    "# 모든 영어 문자는 소문자로 만들고, \n",
    "# 섞여있는 html 태그를 제거하고, \n",
    "# 정규 표현식을 통해 각종 특수문자를 제거한다.\n",
    "\n",
    "# NLTK를 이용\u001f 불용어 제거는 Text 전처리 시에서만 함. 상대적으로 문장 길이가 짧은 Summary 전처리시에는 호출하지 않\u001d",
    ". \n",
    "# Abstractive한 문장 요약의 결과문이 자연스러운 문장이 되려면 불용어들이 Summary에는 남아 있는 게 더 좋을 것 같음. \n",
    "# 이 처리를 위해서 함수의 인자로 remove_stopwords를 추가\u001e",
    " \n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower()                # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)   # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence)             # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n",
    "print('☢️')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee4fabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\t: Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.\n",
      "text\t: everything bought great infact ordered twice third ordered wasfor mother father\n",
      "summary\t: Great way to start (or finish) the day!!!\n",
      "summary\t: great way to start the day\n"
     ]
    }
   ],
   "source": [
    "# preprocess_sentence() 테스트 \n",
    "\n",
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(\"text\\t:\", temp_text)\n",
    "print(\"text\\t:\", preprocess_sentence(temp_text))\n",
    "print(\"summary\\t:\", temp_summary)\n",
    "print(\"summary\\t:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc0b215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 전처리 후 결과:  ['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better', 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo', 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch', 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal', 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터(text부분) 전체에 대하여 전처리 수행 시작\n",
    "    \n",
    "clean_text = []\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(\"Text 전처리 후 결과: \", clean_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "866febde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 전처리 후 결과:  ['good quality dog food', 'not as advertised', 'delight says it all', 'cough medicine', 'great taffy']\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터(summary부분) 전체에 대하여 전처리 수행 시작\n",
    "\n",
    "clean_summary = []\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(\"Summary 전처리 후 결과: \", clean_summary[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e409e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☢️\n"
     ]
    }
   ],
   "source": [
    "# 전처리 후 다시한번 데이터 상태 확인 필요\n",
    "# 빈(empty) 샘플이 생겼는지 확인\n",
    "# 보다 쉽게 확인하기 위해 데이터들을 데이터프레임에 재저장. 빈(empty) 값을 가진 샘플들이 있다면, 모두 Null 값을 가진 샘플로 대체.\n",
    "\n",
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)\n",
    "print('☢️')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b62f4641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null 확인 \n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72b7719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "# Summary 열에서 70개의 Null 발생. \n",
    "# null이 발생한 샘플들은 모두 제거.\n",
    "\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c304cc78",
   "metadata": {},
   "source": [
    "***\n",
    "### 3. 데이터 전처리하기 - (2) 훈련데이터와 테스트데이터 나누기\n",
    "학습을 진행하기 위해서는 학습에 사용할 데이터의 크기를 결정하고, 문장의 시작과 끝을 표시해 주어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a266a90",
   "metadata": {},
   "source": [
    "### ⦿ 샘플의 최대 길이 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6be6d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이\t: 2\n",
      "텍스트의 최대 길이\t: 1235\n",
      "텍스트의 평균 길이\t: 38.792428272310566\n",
      "요약의 최소 길이\t: 1\n",
      "요약의 최대 길이\t: 28\n",
      "요약의 평균 길이\t: 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcwUlEQVR4nO3dfbgWdb3v8fdHUHT7BARxIZgLj5x29qAhKl1ZWe4QH3baOWp6LNBIrtLS9q4Mtp18KK/0tI+W7VIp2aLbNE5mchRDQsjdKRVQEvBhs0Tcgg+gKKCWCX7PH/O7ZViuh2Fg7nvda31e1zXXmvnOb+b+zrplfZ2Z3/xGEYGZmVkZOzU6ATMza14uImZmVpqLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiVhFJr+SmNyX9Obd8eon9HSlpVRW5mpXVt9EJmPVUEbFHbV7SSuALEfHbxmVktuP5TMSsziTtJGmypCckvShphqSBad3Vkm7Ntb1c0lxJuwN3Afvkzmb2adQxmNW4iJjV31eAE4GPAfsALwE/Tuu+Brxf0hmSPgJMBCZExKvAMcAzEbFHmp6pf+pmW/PlLLP6+yLw5YhYBSDpIuA/JX0uIl6T9Dmys46NwFdq7cy6IxcRs/rbD7hN0pu52GZgCLA6Iu6XtAJ4JzCjEQmaFeXLWWb19zRwTET0z027RsRqAEnnAP2AZ4Dzc9t5yG3rdlxEzOrvGuBSSfsBSBos6YQ0/1+B7wKfBT4HnC/p4LTd88A7JO1d/5TN2uciYlZ/PwRmAndL2gjcBxwuqS/wb8DlEfGniFgO/BNwo6R+EfEYcDOwQtLL7p1l3YH8UiozMyvLZyJmZlaai4iZmZXmImJmZqW5iJiZWWm97mHDQYMGRUtLS6PTMDNrGosWLXohIga3t67XFZGWlhYWLlzY6DTMzJqGpKc6WufLWWZmVpqLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlZar3tifXu0TL6zw3UrLzuujpmYmXUPPhMxM7PSKi0iklZKWiJpsaSFKTZQ0hxJy9PPASkuSVdJapX0sKRRuf1MSO2XS5qQix+S9t+atlWVx2NmZlurx5nIxyPi4IgYnZYnA3MjYiQwNy0DHAOMTNMk4GrIig5wIXA4cBhwYa3wpDZn5bYbV/3hmJlZTSMuZ50ATE/z04ETc/EbInMf0F/SUOBoYE5ErIuIl4A5wLi0bq+IuC+yF8XfkNuXmZnVQdVFJIC7JS2SNCnFhkTEs2n+OWBImh8GPJ3bdlWKdRZf1U78bSRNkrRQ0sK1a9duz/GYmVlO1b2zjoiI1ZLeCcyR9Fh+ZUSEpKg4ByJiKjAVYPTo0ZV/nplZb1HpmUhErE4/1wC3kd3TeD5diiL9XJOarwb2zW0+PMU6iw9vJ25mZnVSWRGRtLukPWvzwFhgKTATqPWwmgDcnuZnAuNTL60xwPp02Ws2MFbSgHRDfSwwO63bIGlM6pU1PrcvMzOrgyovZw0Bbku9bvsCP4+I30haAMyQNBF4CjgltZ8FHAu0Aq8BZwJExDpJ3wEWpHaXRMS6NH82cD2wG3BXmszMrE4qKyIRsQI4qJ34i8BR7cQDOKeDfU0DprUTXwi8b7uTNTOzUvzEupmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlaai4iZmZXmImJmZqW5iJiZWWkuImZmVpqLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlaai4iZmZXmImJmZqW5iJiZWWkuImZmVpqLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlplRcRSX0kPSTpjrQ8QtL9klol/ULSLineLy23pvUtuX1MSfHHJR2di49LsVZJk6s+FjMz21o9zkTOAx7NLV8OXBkRBwAvARNTfCLwUopfmdoh6UDgVOC9wDjgJ6kw9QF+DBwDHAicltqamVmdVFpEJA0HjgN+lpYFfAL4ZWoyHTgxzZ+Qlknrj0rtTwBuiYjXI+JJoBU4LE2tEbEiIv4K3JLamplZnVR9JvID4HzgzbT8DuDliNiUllcBw9L8MOBpgLR+fWr/VrzNNh3F30bSJEkLJS1cu3btdh6SmZnVVFZEJB0PrImIRVV9RlERMTUiRkfE6MGDBzc6HTOzHqNvhfv+MPApSccCuwJ7AT8E+kvqm842hgOrU/vVwL7AKkl9gb2BF3Pxmvw2HcXNzKwOKjsTiYgpETE8IlrIbozfExGnA/OAk1KzCcDtaX5mWiatvyciIsVPTb23RgAjgQeABcDI1Ntrl/QZM6s6HjMze7sqz0Q68k3gFknfBR4Crkvx64AbJbUC68iKAhGxTNIM4BFgE3BORGwGkPRlYDbQB5gWEcvqeiRmZr1cXYpIRMwH5qf5FWQ9q9q2+QtwcgfbXwpc2k58FjBrB6ZqZmbbwE+sm5lZaV0WEUknS9ozzX9L0q8kjao+NTMz6+6KnIn8z4jYKOkI4O/I7l1cXW1aZmbWDIoUkc3p53HA1Ii4E9ilupTMzKxZFCkiqyVdC3wGmCWpX8HtzMyshytSDE4h60Z7dES8DAwEvlFlUmZm1hy6LCIR8RqwBjgihTYBy6tMyszMmkOR3lkXkj0gOCWFdgb+rcqkzMysORS5nPVp4FPAqwAR8QywZ5VJmZlZcyhSRP6axrAKAEm7V5uSmZk1iyJFZEbqndVf0lnAb4GfVpuWmZk1gy7HzoqIf5b0SWAD8G7g2xExp/LMzMys2ys0AGMqGi4cZma2lQ6LiKSNpPsgbVcBERF7VZaVmZk1hQ6LSES4B5aZmXWq0OWsNGrvEWRnJr+PiIcqzcrMzJpCkYcNvw1MB94BDAKul/StqhMzM7Pur8iZyOnAQenNg0i6DFgMfLfCvMzMrAkUeU7kGWDX3HI/YHU16ZiZWTMpciayHlgmaQ7ZPZFPAg9IugogIs6tMD8zM+vGihSR29JUM7+aVMzMrNkUeWJ9ej0SMTOz5lOkd9bxkh6StE7SBkkbJW2oR3JmZta9Fbmc9QPgvwFL0mi+ZmZmQLHeWU8DS11AzMysrSJnIucDsyT9Dni9FoyIKyrLyszMmkKRInIp8ArZsyK7VJuOmZk1kyJFZJ+IeF/lmZiZWdMpck9klqSxlWdiZmZNp0gR+RLwG0l/dhdfMzPLK/Kwod8rYmZm7Sr6PpEBwEhyAzFGxL1VJWVmZs2hyBPrXwDuBWYDF6efFxXYbldJD0j6k6Rlki5O8RGS7pfUKukXknZJ8X5puTWtb8nta0qKPy7p6Fx8XIq1Spq8jcduZmbbqcg9kfOAQ4GnIuLjwAeBlwts9zrwiYg4CDgYGCdpDHA5cGVEHAC8BExM7ScCL6X4lakdkg4ETgXeC4wDfiKpj6Q+wI+BY4ADgdNSWzMzq5MiReQvuRdS9YuIx4B3d7VRZF5JizunKYBPAL9M8enAiWn+hLRMWn+UJKX4LRHxekQ8CbQCh6WpNSJWRMRfgVtSWzMzq5MiRWSVpP7Ar4E5km4Hniqy83TGsBhYA8wBngBejohNtX0Dw9L8MLIhVkjr15O9kveteJttOoq3l8ckSQslLVy7dm2R1M3MrIAivbM+nWYvkjQP2Bv4TZGdR8Rm4OBUhG4D/rZkntslIqYCUwFGjx7tMcDMzHaQIjfW/4ukfrVFoAX4m235kIh4GZgHfAjoL6lWvIaz5VW7q4F902f2JStWL+bjbbbpKG5mZnVS5HLWrcBmSQeQ/d/8vsDPu9pI0uB0BoKk3cheq/soWTE5KTWbANye5memZdL6e9LIwTOBU1PvrRFkXY0fABYAI1Nvr13Ibr7PLHA8Zma2gxR5TuTNiNgk6dPAjyLiR5IeKrDdUGB66kW1EzAjIu6Q9Ahwi6TvAg8B16X21wE3SmoF1pEVBSJimaQZwCPAJuCcdJkMSV8m63LcB5gWEcsKHreZme0ARYrIG5JOIztL+PsU27mrjSLiYbLuwG3jK8h6VrWN/wU4uYN9XUo2mnDb+CxgVle5mJlZNYpczjqT7F7GpRHxZLqkdGO1aZmZWTMo0jvrEeDc3PKTpAcBzcysdytyJmJmZtYuFxEzMyutwyIi6cb087z6pWNmZs2kszORQyTtA3xe0gBJA/NTvRI0M7Puq7Mb69cAc4H9gUVkT6vXRIqbmVkv1uGZSERcFRHvIXuIb/+IGJGbXEDMzKxQF98vSToI+EgK3ZseJDQzs16uyACM5wI3Ae9M002SvlJ1YmZm1v0VGfbkC8DhEfEqgKTLgT8CP6oyMTMz6/6KPCciYHNueTNb32Q3M7NeqsiZyL8C90u6LS2fyJaRd83MrBcrcmP9CknzgSNS6MyIKDIUvJmZ9XBFzkSIiAeBByvOxczMmozHzjIzs9JcRMzMrLROi4ikPpLm1SsZMzNrLp0WkfQu8zcl7V2nfMzMrIkUubH+CrBE0hzg1VowIs7teJPep2XynZ2uX3nZcXXKxMysfooUkV+lyczMbCtFnhOZLmk34F0R8XgdcjIzsyZRZADGvwcWA79JywdLmllxXmZm1gSKdPG9CDgMeBkgIhbjF1KZmRnFisgbEbG+TezNKpIxM7PmUuTG+jJJ/wPoI2kkcC7wh2rTMjOzZlDkTOQrwHuB14GbgQ3AVyvMyczMmkSR3lmvARekl1FFRGysPi0zM2sGRXpnHSppCfAw2UOHf5J0SPWpmZlZd1fknsh1wNkR8e8Ako4ge1HVB6pMzMzMur8i90Q21woIQET8HthUXUpmZtYsOiwikkZJGgX8TtK1ko6U9DFJPwHmd7VjSftKmifpEUnLJJ2X4gMlzZG0PP0ckOKSdJWkVkkPp8+u7WtCar9c0oRc/BBJS9I2V0nyu9/NzOqos8tZ/7vN8oW5+Siw703A1yLiQUl7AovSII5nAHMj4jJJk4HJwDeBY4CRaTocuBo4XNLA9Nmj0+cukjQzIl5Kbc4C7gdmAeOAuwrkZmZmO0CHRSQiPr49O46IZ4Fn0/xGSY8Cw4ATgCNTs+lkZzXfTPEbIiKA+yT1lzQ0tZ0TEesAUiEal977vldE3JfiNwAn4iJiZlY3Xd5Yl9QfGA+05Ntvy1DwklqAD5KdMQxJBQbgOWBImh8GPJ3bbFWKdRZf1U68vc+fBEwCeNe73lU0bTMz60KR3lmzgPuAJZQY7kTSHsCtwFcjYkP+tkVEhKQil8a2S0RMBaYCjB49uvLPMzPrLYoUkV0j4h/L7FzSzmQF5KaIqL2T5HlJQyPi2XS5ak2Krwb2zW0+PMVWs+XyVy0+P8WHt9PezMzqpEgX3xslnSVpaOpZNTDd7O5U6il1HfBoRFyRWzUTqPWwmgDcnouPT720xgDr02Wv2cBYSQNST66xwOy0boOkMemzxuf2ZWZmdVDkTOSvwPeBC9jSKyvoejj4DwOfI3vKfXGK/RNwGTBD0kTgKeCUtG4WcCzQCrwGnAkQEeskfQdYkNpdUrvJDpwNXA/sRnZD3TfVzczqqEgR+RpwQES8sC07Tg8ldvTcxlHttA/gnA72NQ2Y1k58IfC+bcnLzMx2nCKXs2pnBmZmZlspcibyKrBY0jyy4eCBbevia2ZmPVORIvLrNJmZmW2lyPtEptcjETMzaz5Fnlh/knbGyoqIrnpnmZlZD1fkctbo3PyuwMlAl8+JmJlZz9dl76yIeDE3rY6IHwDHVZ+amZl1d0UuZ43KLe5EdmZS5AzGzMx6uCLFIP9ekU3ASrY8ZW5mZr1Ykd5Z2/VeETMz67mKXM7qB/x33v4+kUuqS8vMzJpBkctZtwPrgUXknlg3MzMrUkSGR8S4yjMxM7OmU2QAxj9Ien/lmZiZWdMpciZyBHBGenL9dbLh3SMiPlBpZmZm1u0VKSLHVJ6FmZk1pSJdfJ+qRyJmZtZ8itwTMTMza5eLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlaai4iZmZXmImJmZqW5iJiZWWkuImZmVpqLiJmZlVZZEZE0TdIaSUtzsYGS5khann4OSHFJukpSq6SHJY3KbTMhtV8uaUIufoikJWmbqySpqmMxM7P2VXkmcj3Q9t3sk4G5ETESmJuWIXvx1cg0TQKuhqzoABcChwOHARfWCk9qc1ZuO78H3sysziorIhFxL7CuTfgEYHqanw6cmIvfEJn7gP6ShgJHA3MiYl1EvATMAcaldXtFxH0REcANuX2ZmVmd1PueyJCIeDbNPwcMSfPDgKdz7ValWGfxVe3E2yVpkqSFkhauXbt2+47AzMze0rAb6+kMIur0WVMjYnREjB48eHA9PtLMrFeodxF5Pl2KIv1ck+KrgX1z7YanWGfx4e3EzcysjupdRGYCtR5WE4Dbc/HxqZfWGGB9uuw1GxgraUC6oT4WmJ3WbZA0JvXKGp/bl5mZ1UnfqnYs6WbgSGCQpFVkvawuA2ZImgg8BZySms8CjgVagdeAMwEiYp2k7wALUrtLIqJ2s/5ssh5guwF3pcnMzOqosiISEad1sOqodtoGcE4H+5kGTGsnvhB43/bkaGZm28dPrJuZWWkuImZmVpqLiJmZleYiYmZmpVV2Y9221jL5zk7Xr7zsuDplYma24/hMxMzMSnMRMTOz0lxEzMysNBcRMzMrzUXEzMxKcxExM7PSXETMzKw0FxEzMyvNRcTMzEpzETEzs9JcRMzMrDQXETMzK81FxMzMSnMRMTOz0lxEzMysNBcRMzMrzUXEzMxKcxExM7PS/HrcbqKz1+f61blm1l35TMTMzEpzETEzs9JcRMzMrDQXETMzK81FxMzMSnMRMTOz0tzFtwl01v0X3AXYzBrHZyJmZlZa05+JSBoH/BDoA/wsIi5rcEp15wcVzaxRmrqISOoD/Bj4JLAKWCBpZkQ80tjMug9fCjOzKjV1EQEOA1ojYgWApFuAEwAXkYK6KjKdcQEys2YvIsOAp3PLq4DD2zaSNAmYlBZfkfR4ic8aBLxQYrvuZocdhy7fEXsppSd8Fz3hGKBnHEdPOAao9jj262hFsxeRQiJiKjB1e/YhaWFEjN5BKTVMTzgOH0P30ROOoyccAzTuOJq9d9ZqYN/c8vAUMzOzOmj2IrIAGClphKRdgFOBmQ3Oycys12jqy1kRsUnSl4HZZF18p0XEsoo+brsuh3UjPeE4fAzdR084jp5wDNCg41BENOJzzcysB2j2y1lmZtZALiJmZlaai0gBksZJelxSq6TJjc6nI5L2lTRP0iOSlkk6L8UHSpojaXn6OSDFJemqdFwPSxrV2CPYQlIfSQ9JuiMtj5B0f8r1F6kjBZL6peXWtL6loYnnSOov6ZeSHpP0qKQPNdt3Iekf0n9LSyXdLGnXZvguJE2TtEbS0lxsm3/3kiak9sslTegGx/D99N/Tw5Juk9Q/t25KOobHJR2di1f79ysiPHUykd2wfwLYH9gF+BNwYKPz6iDXocCoNL8n8B/AgcD/Aian+GTg8jR/LHAXIGAMcH+jjyF3LP8I/By4Iy3PAE5N89cAX0rzZwPXpPlTgV80OvfcMUwHvpDmdwH6N9N3QfYw75PAbrnv4Ixm+C6AjwKjgKW52Db97oGBwIr0c0CaH9DgYxgL9E3zl+eO4cD0t6kfMCL9zepTj79fDf2PtBkm4EPA7NzyFGBKo/MqmPvtZOOKPQ4MTbGhwONp/lrgtFz7t9o1OO/hwFzgE8Ad6R/3C7l/PG99J2Q98z6U5vumduoGx7B3+gOsNvGm+S7YMiLEwPS7vQM4ulm+C6ClzR/gbfrdA6cB1+biW7VrxDG0Wfdp4KY0v9Xfpdp3UY+/X76c1bX2hlYZ1qBcCkuXEj4I3A8MiYhn06rngCFpvrse2w+A84E30/I7gJcjYlNazuf51jGk9etT+0YbAawF/jVdlvuZpN1pou8iIlYD/wz8J/As2e92Ec33XdRs6+++230nbXye7AwKGngMLiI9kKQ9gFuBr0bEhvy6yP53pNv265Z0PLAmIhY1Opft1JfsUsTVEfFB4FWySyhvaYLvYgDZgKYjgH2A3YFxDU1qB+nuv/uuSLoA2ATc1OhcXES61lRDq0jamayA3BQRv0rh5yUNTeuHAmtSvDse24eBT0laCdxCdknrh0B/SbWHY/N5vnUMaf3ewIv1TLgDq4BVEXF/Wv4lWVFppu/i74AnI2JtRLwB/Irs+2m276JmW3/33fE7QdIZwPHA6akYQgOPwUWka00ztIokAdcBj0bEFblVM4Faz5IJZPdKavHxqXfKGGB97nS/ISJiSkQMj4gWst/1PRFxOjAPOCk1a3sMtWM7KbVv+P9hRsRzwNOS3p1CR5G9oqBpvguyy1hjJP1N+m+rdgxN9V3kbOvvfjYwVtKAdFY2NsUaRtlL+M4HPhURr+VWzQROTT3kRgAjgQeox9+vet4kataJrPfGf5D1crig0fl0kucRZKfoDwOL03Qs2XXpucBy4LfAwNReZC/1egJYAoxu9DG0OZ4j2dI7a//0j6IV+D9AvxTfNS23pvX7NzrvXP4HAwvT9/Frsh4+TfVdABcDjwFLgRvJev90++8CuJnsPs4bZGeFE8v87snuO7Sm6cxucAytZPc4av++r8m1vyAdw+PAMbl4pX+/POyJmZmV5stZZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlaai4j1WJJeqWCfB0s6Nrd8kaSvb8f+Tk4j/M7bMRmWzmOlpEGNzMGak4uI2bY5mKzf/Y4yETgrIj6+A/dpVjcuItYrSPqGpAXpPQwXp1hLOgv4aXpnxt2SdkvrDk1tF6d3OCxNT/xeAnwmxT+Tdn+gpPmSVkg6t4PPP03SkrSfy1Ps22QPiF4n6ftt2g+VdG/6nKWSPpLiV0tamPK9ONd+paTvpfYLJY2SNFvSE5K+mNocmfZ5Z3q/xDWS3vY3QNJnJT2Q9nWtsne79JF0fcpliaR/2M6vxHqKRj8R68lTVRPwSvo5FphK9mTyTmRDmn+UbJjtTcDBqd0M4LNpfilbhjW/jDQcN9n7NP4l9xkXAX8ge5J7ENlYUTu3yWMfsiFEBpMNzHgPcGJaN592nk4HvkZ6upjsnRB7pvmBudh84ANpeSVb3utxJdlT8numz3w+xY8E/kL2xHkfYA5wUm77QcB7gP9bOwbgJ8B44BBgTi6//o3+fj11j8lnItYbjE3TQ8CDwN+SjS0E2QCDi9P8IqBF2dvi9oyIP6b4z7vY/50R8XpEvEA2qN+QNusPBeZHNpBhbeTVj3axzwXAmZIuAt4fERtT/BRJD6ZjeS/Zy4hqamMiLSF7sdLGiFgLvK4tb8B7ICJWRMRmsmE1jmjzuUeRFYwFkhan5f3JXsi0v6QfpfGbNmBG9n9FZj2dgO9FxLVbBbN3rryeC20Gdiux/7b72O5/VxFxr6SPAscB10u6Avh34OvAoRHxkqTrycarapvHm21yejOXU9txjtouC5geEVPa5iTpILKXUn0ROIVsXCnr5XwmYr3BbODzyt6zgqRhkt7ZUeOIeBnYKOnwFDo1t3oj2WWibfEA8DFJgyT1IXtj3u8620DSfmSXoX4K/IxsGPm9yN5Lsl7SEOCYbcwD4LA0outOwGeA37dZPxc4qfb7UfZe8v1Sz62dIuJW4FspHzOfiVjPFxF3S3oP8MdsRHNeAT5LdtbQkYnATyW9SfYHf32KzwMmp0s93yv4+c9Kmpy2Fdnlr9u72OxI4BuS3kj5jo+IJyU9RDaq7tPA/yvy+W0sAP4FOCDlc1ubXB+R9C3g7lRo3gDOAf5M9pbG2v94vu1MxXonj+Jr1g5Je0TEK2l+Mtm7uc9rcFrbRdKRwNcj4vgGp2I9iM9EzNp3nKQpZP9GniLrlWVmbfhMxMzMSvONdTMzK81FxMzMSnMRMTOz0lxEzMysNBcRMzMr7f8Do1dsKbWvtPIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text와 Summary의 최소, 최대, 평균 길이를 구하고 또한 길이 분포를 시각화\n",
    "# 길이 분포 출력\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이\\t: {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이\\t: {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이\\t: {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이\\t: {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이\\t: {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이\\t: {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)       # 1X2의 1번째에 그린다.\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)       # 1X2의 2번째에 그린다.\n",
    "plt.boxplot(summary_len) # 평균, 최솟값, 최댓값을 보여준다,.\n",
    "plt.title('Summary')\n",
    "plt.tight_layout()       # 자동으로 subplot간의 간격을 조절 한다. \n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ddc5bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "#  Text의 최대 길이와 Summary의 적절한 최대 길이를 임의로 정함\n",
    "\n",
    "text_max_len = 50\n",
    "summary_max_len = 8\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2da24b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 샘플의 길이를 입력하면, 데이터의 몇 %가 해당하는지 계산하는 함수\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율 : %s'%(max_len, (cnt / len(nested_list))))\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8caaef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율 : 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율 : 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a80d375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 65818\n"
     ]
    }
   ],
   "source": [
    "# 정해진 길이보다 길면 제외하는 방법으로 데이터를 정제\n",
    "\n",
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc7111",
   "metadata": {},
   "source": [
    "### ⦿ 시작 토큰과 종료 토큰 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00d057a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "# sostoken : 시작 토큰\n",
    "# eostoken : 종료 토큰 \n",
    "# decoder_input  : 디코더의 입력에 해당하면서 시작 토큰이 맨 앞에 있는 문장의 이름\n",
    "# decoder_target : 디코더의 출력 또는 레이블에 해당되면서 종료 토큰이 맨 뒤에 붙는 문장의 이름 \n",
    "\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fdf5e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더의 입력, 디코더의 입력, 디코더의 레이블을 각각 Numpy 타입으로 저장(지정)\n",
    "    \n",
    "encoder_input = np.array(data['Text'])            # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input'])   # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "666582ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36344 19121 56452 ... 56636 42882 63688]\n",
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 테스트 데이터를 분리하는 코딩을 직접 작성\n",
    "\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "\n",
    "# 위에서 만든 정수 시퀀스를 이용해 다시 데이터의 샘플 순서를 정의해 주면 잘 섞인 샘플이 \u001d",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba9c6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13163\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터의 수를 20%로 정하고 해당 수를 구한다. \n",
    "\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "777e2cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52655\n",
      "훈련 레이블의 개수 : 52655\n",
      "테스트 데이터의 개수 : 13163\n",
      "테스트 레이블의 개수 : 13163\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c1bdd",
   "metadata": {},
   "source": [
    "***\n",
    "### 4. 데이터 전처리하기 - (3) 정수 인코딩\n",
    "이제 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 모두 정수로 바꾸어 주어야 해요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9dce0",
   "metadata": {},
   "source": [
    "### ⦿ 단어 집합(vocabulary) 만들기 및 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "110474e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# Keras의 토크나이저를 사용하면, 입력된 훈련 데이터로부터 단어 집합을 만들 수 있어요.\n",
    "\n",
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c075ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기      : 31876\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수 : 23619\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 : 8257\n",
      "단어 집합에서 희귀 단어의 비율           : 74.09649893336679\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율 : 3.3670036319850785\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합에 있는 모든 단어를 사용하는 것이 아니라, 빈도수가 낮은 단어들은 훈련 데이터에서 제외하고 진행\n",
    "# src_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장\n",
    "\n",
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기      :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수 : %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 : %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율           :\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율 :\", (rare_freq / total_freq)*100)\n",
    "\n",
    "\n",
    "# 등장 빈도가 threshold 값인 7회 미만, 즉 6회 이하인 단어들은 단어 집합에서 무려 70% 이상을 차지하네요. \n",
    "# 하지만 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 3.39%밖에 되지 않아요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef8a5c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거\n",
    "\n",
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f60ae8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48, 3201, 7207, 736, 409, 316, 128, 4084, 3, 117, 598, 3468, 17, 414, 234, 3307, 731, 5095, 322], [1002, 262, 209, 5269, 3584, 122, 100, 680, 607, 123, 2192, 1243, 2065, 1607, 149, 3585, 3586, 6185, 533, 37, 149, 84, 262, 3, 16, 1, 274, 262, 25, 17, 453, 303, 1631], [91, 152, 52, 5, 366, 1555, 152, 78, 1409, 26, 164, 395, 1005]]\n"
     ]
    }
   ],
   "source": [
    "# texts_to_sequences()는 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩을 수행\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "273a10f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# Summary 데이터에 대해서도 동일한 작업을 수행\n",
    "# 케라스의 토크나이저를 사용하여 decoder_input_train을 입력으로 전체 단어 집합과 각 단어에 대한 빈도수를 계산\n",
    "    \n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddff9b",
   "metadata": {},
   "source": [
    "### ⦿ 중간 점검 \n",
    "- 이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되, tar_tokenizer.word_index에 저장되어 있어요\n",
    "- tar_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cb45802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10441\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8053\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2388\n",
      "단어 집합에서 희귀 단어의 비율: 77.12862752609904\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.844530328722552\n"
     ]
    }
   ],
   "source": [
    "# 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인\n",
    "\n",
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "\n",
    "# 등장 빈도가 5회 이하인 단어들은 단어 집합에서 약 77%를 차지하고 있네요. \n",
    "# 하지만 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 수치인 5.87%밖에 되지 않아요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "946b5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 24, 60, 8, 268, 10], [1, 1797, 203], [1, 80, 53, 656], [1, 4, 17], [1, 502, 697, 235]]\n",
      "target\n",
      "decoder  [[24, 60, 8, 268, 10, 2], [1797, 203, 2], [80, 53, 656, 2], [4, 17, 2], [502, 697, 235, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 등장 빈도가 5회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거\n",
    "\n",
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b1630",
   "metadata": {},
   "source": [
    "### ⦿ 잠깐만 \n",
    "전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 이제 빈(empty) 샘플이 되었을 가능성이 있어요\n",
    "평균 길이가 4밖에 되지 않았던 요약문(Summary)의 경우에는 이 현상이 굉장히 두드러졌을 가능성이 높겠죠.\n",
    "요약문인 decoder_input에는 sostoken 또는 decoder_target에는 eostoken이 추가된 상태이고, \n",
    "이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플 수와 동일하게 매우 높으므로 단어 집합 제한에도 삭제되지 않았어요. \n",
    "그래서 이제 길이가 0이 된 요약문의 실제 길이는 1로 나올 것임.\n",
    "길이 0이 된 decoder_input에는 sostoken, decoder_target에는 eostoken만 남아 있을 테니까요.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1311710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1238\n",
      "삭제할 테스트 데이터의 개수 : 363\n",
      "훈련 데이터의 개수 : 51417\n",
      "훈련 레이블의 개수 : 51417\n",
      "테스트 데이터의 개수 : 12800\n",
      "테스트 레이블의 개수 : 12800\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 1인 경우의 인덱스를 각각 drop_train과 drop_test에 라는 변수에 저장\n",
    "# 이 샘플들은 모두 삭제할 것임.\n",
    "\n",
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda7546",
   "metadata": {},
   "source": [
    "### ⦿ 패딩하기\n",
    "텍스트 시퀀스를 정수 시퀀스로 변환했다면, 이제 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업필요<br>최대 길이에 맞도록 최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 최대 길이로 길이를 맞춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81c3d681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ff52c",
   "metadata": {},
   "source": [
    "### ⦿ 데이터 전처리 완료 !!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3bdd89",
   "metadata": {},
   "source": [
    "### 5. 모델 설계하기 (인코더 + 디코더)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "688dea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# 함수형 API를 이용해서 인코더를 설계\n",
    "    \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256   # hidden state는 LSTM에서 얼만큼의 수용력(capacity)를 가질지를 정하는 파라미터\n",
    "                    # 이 파라미터는 LSTM의 용량의 크기나, LSTM에서의 뉴런의 개수라 이해하자.\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, \n",
    "                     return_sequences=True, \n",
    "                     return_state=True ,\n",
    "                     dropout = 0.4,           # dropout은 레이어의 weight를 랜덤으로 생략하여 모델의 과적합(overfitting)을 해결해주는 방법\n",
    "                     recurrent_dropout = 0.4) # recurrent dropout은 dropout을 레이어가 아닌 time step마다 해주는 방식\n",
    "                                              # ,즉 time step의 입력을 랜덤으로 생략해 주는 효과가 있음.\n",
    "                                              # recurrent dropout은 일반적인 dropout와 같이 regularization을 해주는 효과가 있고, \n",
    "                                              # 과적합을 방지할 수 있다고 해요\n",
    "                                              # dropout과 recurrent dropout을 모두 사용한 것을 Variational Dropout이라 함.\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0a85582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "# 디코더 설계시 LSTM의 입력을 정의할 때, \n",
    "# initial_state의 인자값으로 인코더의 hidden state와 cell state의 값을 넣어줘야 해요\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04bd24d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b57d94",
   "metadata": {},
   "source": [
    "⦿ 지금까지 설계한 것은 인코더의 hidden state와 cell state를 디코더의 초기 state로 사용하는 가장 기본적인 seq2seq<br>\n",
    "⦿ 디코더의 출력층을 설계를 살짝 바꿔서 성능을 높일 수 있는 방법 => 어텐션 메커니즘이에요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d95830",
   "metadata": {},
   "source": [
    "### ⦿ 어텐션 메커니즘\n",
    "어텐션 메커니즘을 수행하는 어텐션 함수를 설계하는 것은 또 다른 새로운 신경망을 설계해야 한다는 뜻<br>\n",
    "여기서 사용하는 어텐션 함수는 Bahdanau 스타일의 어텐션입니다    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8069927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_3[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,145,360\n",
      "Trainable params: 4,145,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# 인코더의 hidden state들과 디코더의 hidden state들을 어텐션 함수의 입력으로 사용하고, \n",
    "# 어텐션 함수가 리턴한 값을 예측 시에 디코더의 hidden state와 함께 활용하는 형태로 작동"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc39ffd",
   "metadata": {},
   "source": [
    "### 6. 모델 훈련 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd079597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "201/201 [==============================] - 147s 648ms/step - loss: 2.7019 - val_loss: 2.4059\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 2.3681 - val_loss: 2.2557\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 128s 639ms/step - loss: 2.2166 - val_loss: 2.1453\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 128s 639ms/step - loss: 2.1060 - val_loss: 2.0590\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 128s 639ms/step - loss: 2.0206 - val_loss: 2.0019\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 1.9554 - val_loss: 1.9610\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 128s 639ms/step - loss: 1.9030 - val_loss: 1.9392\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 128s 637ms/step - loss: 1.8593 - val_loss: 1.9153\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 128s 637ms/step - loss: 1.8214 - val_loss: 1.9002\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 1.7871 - val_loss: 1.8862\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 128s 635ms/step - loss: 1.7553 - val_loss: 1.8758\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 128s 636ms/step - loss: 1.7253 - val_loss: 1.8683\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 128s 637ms/step - loss: 1.6969 - val_loss: 1.8613\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 128s 636ms/step - loss: 1.6716 - val_loss: 1.8591\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 128s 638ms/step - loss: 1.6467 - val_loss: 1.8542\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 128s 637ms/step - loss: 1.6216 - val_loss: 1.8516\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 128s 636ms/step - loss: 1.5981 - val_loss: 1.8524\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 128s 634ms/step - loss: 1.5764 - val_loss: 1.8513\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 128s 636ms/step - loss: 1.5550 - val_loss: 1.8524\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 127s 634ms/step - loss: 1.5355 - val_loss: 1.8517\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# '조기 종료'를 뜻하는 EarlyStopping은 특정 조건이 충족되면 훈련을 멈추는 역할\n",
    "# 아래 코드에서는 val_loss(검증 데이터의 손실)을 관찰하다가, \n",
    "# 검증 데이터의 손실이 줄어들지 않고 증가하는 현상이 2회(patience=2) 관측되면 학습을 멈추도록 설정됨\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "                    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "                    batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380a208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
